{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install required packages if needed. Refer to README for more info.\n",
        "!pip install requests pandas gdown"
      ],
      "metadata": {
        "id": "Uc1aLMw34I-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#COOKIES\n",
        "!mkdir -p ~/.cache/gdown\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "!mkdir -p ~/.cache/gdown && mv /content/cookies.txt ~/.cache/gdown/cookies.txt"
      ],
      "metadata": {
        "id": "Oah7uos-qNSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jt7BPWrvoDCC"
      },
      "outputs": [],
      "source": [
        "#SUMMARY_CSV_COMPILER\n",
        "import os\n",
        "import time\n",
        "import requests\n",
        "import pandas as pd\n",
        "import gdown\n",
        "import re\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "def list_files_in_folder(folder_id, api_key):\n",
        "    \"\"\"\n",
        "    Lists all CSV files in the Google Drive folder using the Drive API.\n",
        "    Returns a list of dictionaries with 'id' and 'name' for each file.\n",
        "    This program requires an input of the folder ID and API key, which are\n",
        "    passed in through main by the user.\n",
        "    \"\"\"\n",
        "\n",
        "    files_list = []\n",
        "\n",
        "    page_token = None\n",
        "    base_url = \"https://www.googleapis.com/drive/v3/files\"\n",
        "    query = f\"'{folder_id}' in parents and mimeType='text/csv'\"\n",
        "\n",
        "\n",
        "    while True:\n",
        "        params = {\n",
        "            \"q\": query,\n",
        "            \"fields\": \"nextPageToken, files(id, name)\",\n",
        "            \"pageToken\": page_token,\n",
        "            \"key\": api_key\n",
        "        }\n",
        "        response = requests.get(base_url, params=params)\n",
        "        if response.status_code != 200:\n",
        "            print(\"Error fetching file list.\")\n",
        "            return files_list\n",
        "\n",
        "        data = response.json()\n",
        "        files_list.extend(data.get(\"files\", []))\n",
        "        page_token = data.get(\"nextPageToken\")\n",
        "        if not page_token:\n",
        "            break\n",
        "    return files_list\n",
        "\n",
        "\n",
        "def find_files_with_age_type_2(dataset_dir):\n",
        "    \"\"\"\n",
        "    Looks through each CSV file in the dataset directory.\n",
        "    If the file contains 'age_type' == 2, its filename is added to a list.\n",
        "    This is because 'age_type' == 2 indicates that the battery was cyclically aged,\n",
        "    representing the data we are interested in. The function inputs the dataset directory path\n",
        "    and outputs a list of filenames that meet the condition.\n",
        "    \"\"\"\n",
        "\n",
        "    #Creates a list to store cyclic file names to\n",
        "    age_type_2_files = []\n",
        "    #For each file in the dataset, we check that it is csv, find if its age type\n",
        "    #is 2, and then append its filename to a list if so.\n",
        "    for filename in os.listdir(dataset_dir):\n",
        "        if filename.endswith('.csv'):\n",
        "            file_path = os.path.join(dataset_dir, filename)\n",
        "            try:\n",
        "                df = pd.read_csv(file_path, sep=';')\n",
        "            except Exception as e:\n",
        "                continue\n",
        "            if 'age_type' in df.columns and (df['age_type'] == 2).any():\n",
        "                age_type_2_files.append(filename)\n",
        "    return age_type_2_files\n",
        "\n",
        "def collect_data(dataset_dir, file_list):\n",
        "  \"\"\"\n",
        "    For each CSV file in file_list (which is passed in via main and represents files\n",
        "    with 'age_type' == 2' compiled from the find_files_with_age_type_2 function), we do the following:\n",
        "        1.) Check for a row where `soh_cap` <= 66.6666667. This represents when the battery reaches 80%\n",
        "           EOL capacity or below, which is our threshold for battery end-of-life. The reason we use\n",
        "           66.6667 is because the dataset normalizes data so that \"EOL\" of 40% capacity fade is reached\n",
        "           when soh_cap is 0. 66.6667 soh_cap is equal to an 80% capacity fade from 100%.\n",
        "              If no such row exists, we skip the file/battery from our analysis. This is because the battery\n",
        "              would be an outlier and impossible to quantify its end-of-life parameters.\n",
        "        2.) For the first row EOL is reached, we record its `num_cycles_op`, which indicates the number\n",
        "           of cycles taken before the battery reached EOL.\n",
        "        3.) Then, we compute averages for the columns that are significant for our analysis.\n",
        "        4.) We finally return a dictionary with the filename and the stored data for the necessary metrics from each significant column.\n",
        "    This code inputs the file_list detailed above, the dataset directory path, and outputs a dictionary with each batteries'\n",
        "    filename, average v_min_target_V, average v_max_target_V, average age_temp, average age_chg_rate, and average age_dischg_rate.\n",
        "    \"\"\"\n",
        "\n",
        "  #Creates a dictionary for us to store our necessary information\n",
        "  result_dict = {}\n",
        "\n",
        "  #For each file, we check to ensure soh_cap reaches 66.6667 or below.\n",
        "  #The file is skkipped if not, as this would be an outlying cell that does\n",
        "  #Not meet EOL requirements (which is abnorma).\n",
        "  for filename in file_list:\n",
        "      file_path = os.path.join(dataset_dir, filename)\n",
        "      df = pd.read_csv(file_path, sep=';')\n",
        "      mask = df['soh_cap'] <= 66.6666667\n",
        "      if not mask.any():\n",
        "          print(f\"Skipped {filename}.\")\n",
        "          continue\n",
        "\n",
        "  #finds average column values for important variables we will need later\n",
        "      file_result = {\"num_cycles_op\": df.loc[mask, 'num_cycles_op'].iloc[0]}\n",
        "      if 'v_min_target_V' in df.columns:\n",
        "          file_result['avg_v_min_target_V'] = df['v_min_target_V'].mean()\n",
        "      if 'v_max_target_V' in df.columns:\n",
        "          file_result['avg_v_max_target_V'] = df['v_max_target_V'].mean()\n",
        "      if 'age_temp' in df.columns:\n",
        "          file_result['avg_age_temp'] = df['age_temp'].mean()\n",
        "      if 'age_chg_rate' in df.columns:\n",
        "          file_result['avg_age_chg_rate'] = df['age_chg_rate'].mean()\n",
        "      if 'age_dischg_rate' in df.columns:\n",
        "        file_result['avg_age_dischg_rate'] = df['age_dischg_rate'].mean()\n",
        "\n",
        "      result_dict[filename] = file_result\n",
        "\n",
        "  return result_dict\n",
        "\n",
        "def add_soc_window_columns(df):\n",
        "    \"\"\"\n",
        "    This function takes the dictionary output collected from the collect_data function (which is converted into a pandas dataframe in main\n",
        "    and adds two columns to the DataFrame: SOC Window Min and SOC Window Max. Essentially, this allows us to determine the state of charge\n",
        "    (SOC) window for each battery tested based on comparing the average v_min_target_V and v_max_target_V values to SOC profiles determined\n",
        "    by voltage as specified in the dataset source (3.249, 2.500, 4.200, 4.092). Slight errors of +-0.1 and +-0.05 are included to account for experimental error.\n",
        "    There is greateer variance in the SOC Window Min values, so the error range is 0.05 larger than for SOC Window Max. Additionally, any value that\n",
        "    does not match the specified values is set to 50, which is a default value indicating that the SOC window is not known, indicating that the\n",
        "    user should mannually decide wether to include/disclude the particular battery from analysis. Finally, the SOC Window Min and SOC Window Max values are added to\n",
        "    the dataframe and returned.\n",
        "    \"\"\"\n",
        "\n",
        "    #This subfunction finds the soc_min with an error of 0.1 to account for experimental discrepancies\n",
        "    def get_soc_window_min(avg_v_min):\n",
        "        if abs(avg_v_min - 3.249) <= 0.1:\n",
        "            return 10\n",
        "        elif abs(avg_v_min - 2.500) <= 0.1:\n",
        "            return 0\n",
        "       #If no soc_min is found in the range, the function returns 50. This is so that the user\n",
        "       #Can easily pinpoint the file in the summary.csv file and decide how to tune the parameters\n",
        "       #to better fit the datapoint. For our dataset, 50 should never be returned, but it may be useful\n",
        "       #for users who have issues in their own custom datasets.\n",
        "        else:\n",
        "            return 50\n",
        "  #This subfunction finds the soc_max with an error of 0.05 to account for experimental discrepancies.\n",
        "    def get_soc_window_max(avg_v_max):\n",
        "        if abs(avg_v_max - 4.200) <= 0.05:\n",
        "            return 100\n",
        "        elif abs(avg_v_max - 4.092) <= 0.05:\n",
        "            return 90\n",
        "      #If no soc_max is found in the range, the function returns 50. This is so that the user\n",
        "       #Can easily pinpoint the file in the summary.csv file and decide how to tune the parameters\n",
        "       #to better fit the datapoint. For our dataset, 50 should never be returned, but it may be useful\n",
        "       #for users who have issues in their own custom datasets.\n",
        "        else:\n",
        "            return 50\n",
        "#Here we call the subfunctions and use them to add the soc windows to the data frame\n",
        "    if 'avg_v_min_target_V' in df.columns:\n",
        "        df['SOC Window Min'] = df['avg_v_min_target_V'].apply(get_soc_window_min)\n",
        "    if 'avg_v_max_target_V' in df.columns:\n",
        "        df['SOC Window Max'] = df['avg_v_max_target_V'].apply(get_soc_window_max)\n",
        "    return df\n",
        "\n",
        "def extract_folder_id(folder_link):\n",
        "    \"\"\"\n",
        "    Extracts the folder ID from a Google Drive folder shareable URL.\n",
        "    \"\"\"\n",
        "    match = re.search(r'folders/([a-zA-Z0-9_-]+)', folder_link)\n",
        "    if match:\n",
        "        return match.group(1)\n",
        "    else:\n",
        "        print(\"Error finding folder ID.\")\n",
        "        return \"\"\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function that downloads all CSV files from a specified Google Drive folder,\n",
        "    processes them, and saves the resulting summary to a CSV file.\n",
        "\n",
        "    Steps:\n",
        "    1. Prompt the user for a shareable Google Drive folder link and a Google API key.\n",
        "    2. Extract the folder ID from the provided link.\n",
        "    3. Use the Drive API to fetch a list of all CSV files in the folder.\n",
        "    4. Delete any existing temporary directory (/content/temp_csvs) and recreate it.\n",
        "    5. Download each CSV file using gdown while printing the current\n",
        "       file number, total files, and an estimated time remaining.\n",
        "    6. Pause briefly between downloads to mitigate rate limiting.\n",
        "    7. Process the downloaded files to select only those that have 'age_type' equal to 2.\n",
        "    8. Collect relevant data from these files and add additional SOC window columns.\n",
        "    9. Save the aggregated data as \"summary.csv\" in the /content directory and trigger its download.\n",
        "    \"\"\"\n",
        "    folder_link = input(\"Enter your Google Drive folder shareable link (containing the CSV files): \").strip()\n",
        "    api_key = input(\"Enter your Google API key (folder must be publicly shared): \").strip()\n",
        "    folder_id = extract_folder_id(folder_link)\n",
        "    if not folder_id:\n",
        "        return\n",
        "\n",
        "    print(\"Fetching file list from Google Drive API...\")\n",
        "    files_list = list_files_in_folder(folder_id, api_key)\n",
        "    total_files = len(files_list)\n",
        "    print(f\"Found {total_files} CSV files in the folder.\")\n",
        "\n",
        "    tmp_dir = '/content/temp_csvs'\n",
        "    if os.path.exists(tmp_dir):\n",
        "        shutil.rmtree(tmp_dir)\n",
        "    os.makedirs(tmp_dir, exist_ok=True)\n",
        "\n",
        "    download_times = []\n",
        "    downloaded_count = 0\n",
        "    for idx, file_info in enumerate(files_list, start=1):\n",
        "        file_id = file_info['id']\n",
        "        file_name = file_info['name']\n",
        "        if not file_name.lower().endswith('.csv'):\n",
        "            file_name += '.csv'\n",
        "        download_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "        output_path = os.path.join(tmp_dir, file_name)\n",
        "        start_time = time.time()\n",
        "        try:\n",
        "            gdown.download(download_url, output_path, quiet=True, fuzzy=True)\n",
        "            downloaded_count += 1\n",
        "        except Exception:\n",
        "            print(f\"Failed to download {file_name}.\")\n",
        "            continue\n",
        "        elapsed = time.time() - start_time\n",
        "        download_times.append(elapsed)\n",
        "        avg_time = sum(download_times) / len(download_times) if download_times else 0\n",
        "        remaining = total_files - idx\n",
        "        est_time_remaining = remaining * avg_time\n",
        "        print(f\"Downloaded {idx} / {total_files} CSV Files. Estimated Time Remaining: {est_time_remaining:.1f} seconds\")\n",
        "        time.sleep(0.5) #This adds a 0.5 second delay after each download. This way we can bypass Google's rate/bandwidth\n",
        "        # regulations/limits. Otherwise, google stops us from downloading files and the program fails.\n",
        "    print(f\"Finished downloading. Successfully downloaded {downloaded_count} files out of {total_files}.\")\n",
        "\n",
        "    filtered_files = find_files_with_age_type_2(tmp_dir)\n",
        "    data_dict = collect_data(tmp_dir, filtered_files)\n",
        "    print(f\"{len(data_dict)} files meet all conditions.\")\n",
        "    if not data_dict:\n",
        "        print(\"No files met the conditions.\")\n",
        "        return\n",
        "\n",
        "    df = pd.DataFrame.from_dict(data_dict, orient='index')\n",
        "    df.index.name = 'filename'\n",
        "    df = add_soc_window_columns(df)\n",
        "\n",
        "    summary_csv_path = os.path.join('/content', \"summary.csv\")\n",
        "    df.to_csv(summary_csv_path)\n",
        "    print(f\"summary.csv saved to: {summary_csv_path}\")\n",
        "    files.download(summary_csv_path)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}